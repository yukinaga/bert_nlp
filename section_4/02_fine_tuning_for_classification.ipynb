{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN8EhHHFQLNkez0IDtYoIZH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yukinaga/bert_nlp/blob/main/section_4/02_fine_tuning_for_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrgegdDZjf8E"
      },
      "source": [
        "## ファインチューニングによる感情分析\n",
        "ファインチューニングを活用し、文章の好悪感情を判別できるようにモデルを訓練します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6moZnLFkFwr"
      },
      "source": [
        "## ライブラリのインストール\n",
        "ライブラリTransformers、およびnlpをインストールします。  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7qg6t5nnBjqs"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install nlp\n",
        "!pip install dill==0.3.5\n",
        "!pip install accelerate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsgQNMJxpBnW"
      },
      "source": [
        "## モデルとTokenizerの読み込み\n",
        "事前学習済みのモデルと、これと紐づいたTokenizerを読み込みます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9R0HK29fHrf3"
      },
      "source": [
        "from transformers import BertForSequenceClassification, BertTokenizerFast\n",
        "\n",
        "sc_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\")\n",
        "sc_model.cuda()\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWCmm2TjqToE"
      },
      "source": [
        "## データセットの読み込み\n",
        "ライブラリnlpを使用して、IMDbデータセットを読み込みます。  \n",
        "IMDbデータセットは、25000の映画レビューコメントに、ポジティブかネガティブの好悪感情を表すラベルが付随した、感情分析用のデータセットです。  \n",
        "https://www.imdb.com/interfaces/  \n",
        "読み込んだIMDbのデータはトークナイザーで処理し、形式を整えます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfEnNpv9HuXI"
      },
      "source": [
        "from nlp import load_dataset\n",
        "\n",
        "def tokenize(batch):\n",
        "    return tokenizer(batch[\"text\"], padding=True, truncation=True)\n",
        "\n",
        "train_data, test_data = load_dataset(\"imdb\", split=[\"train\", \"test\"])\n",
        "print(train_data[\"label\"][0], train_data[\"text\"][0])  # 好意的なコメント\n",
        "print(train_data[\"label\"][20000], train_data[\"text\"][20000])  # 否定的なコメント\n",
        "\n",
        "train_data = train_data.map(tokenize, batched=True, batch_size=len(train_data))\n",
        "train_data.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
        "\n",
        "test_data = test_data.map(tokenize, batched=True, batch_size=len(train_data))\n",
        "test_data.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Y6Fcqmy2rG2"
      },
      "source": [
        "## 評価用の関数\n",
        "`sklearn.metrics`を使用し、モデルを評価するための関数を定義します。  \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plAZjdkG0FdV"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def compute_metrics(result):\n",
        "    labels = result.label_ids\n",
        "    preds = result.predictions.argmax(-1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "    }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjLqAVy7z0T3"
      },
      "source": [
        "## Trainerの設定\n",
        "Trainerクラス、およびTrainingArgumentsクラスを使用して、訓練を行うTrainerの設定を行います。\n",
        "https://huggingface.co/transformers/main_classes/trainer.html   \n",
        "https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhaexaAOI3kV"
      },
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir = \"./results\",\n",
        "    num_train_epochs = 1,\n",
        "    per_device_train_batch_size = 8,\n",
        "    per_device_eval_batch_size = 32,\n",
        "    per_gpu_train_batch_size = 8,\n",
        "    warmup_steps = 500,  # 学習係数が0からこのステップ数で上昇\n",
        "    weight_decay = 0.01,  # 重みの減衰率\n",
        "    # evaluate_during_training = True,  # ここの記述はバージョンによっては必要ありません\n",
        "    logging_dir = \"./logs\",\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model = sc_model,\n",
        "    args = training_args,\n",
        "    compute_metrics = compute_metrics,\n",
        "    train_dataset = train_data,\n",
        "    eval_dataset = test_data\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0F5nXKpSCnS"
      },
      "source": [
        "## モデルの訓練\n",
        "設定に基づきモデルを訓練します。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29fkN4UcI4jm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138
        },
        "outputId": "d757de7b-af45-499e-8aed-9d9f0b5a5cb4"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1176' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1176/3125 15:01 < 24:56, 1.30 it/s, Epoch 0.38/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.455500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.359300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c76zhkQVS2xZ"
      },
      "source": [
        "## モデルの評価\n",
        "Trainerの`evaluate()`メソッドによりモデルを評価します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wIgke21zI6l_"
      },
      "source": [
        "trainer.evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EFwqzLRUhaB"
      },
      "source": [
        "## TensorBoardによる結果の表示\n",
        "TensorBoardを使って、logsフォルダに格納された学習過程を表示します。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1vv39tuDJq5n"
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}